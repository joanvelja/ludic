# PROVER-VERIFIER GAMES IMPROVE LEGIBILITY OF LLM OUTPUTS
Jan Hendrik Kirchner\* Yining Chen\*
Harri Edwards† Jan Leike† Nat McAleese Yuri Burda†
OpenAI

## 1 INTRODUCTION
As machine learning systems find uses in higher stakes areas, it becomes increasingly important to justify why we can trust their outputs and delineate when such trust should not be placed. One possible way to gain trust in the output of a complex system is to require that the system produces an explanation of its output that is *legible* to humans or another trusted system, i.e. can be fully understood to the extent that any possible errors would be spotted. For example, to build trust in the judicial system we ask courts to produce clear and legible written opinions that explain and support their decision making.
In this paper we ask whether we can make outputs of capable LLMs more legible to humans. One direct approach is to optimize model outputs for legibility via human feedback (e.g., RLHF; Christiano et al. 2017), which has its limitations (Casper et al., 2023). We study a complementary approach, and ask whether optimizing larger LLM provers to produce Chain-of-Thought (CoT) solutions *checkable* by smaller LLM verifiers makes them more *legible* to humans. Our motivations for studying smaller verifiers as a proxy for humans include: (1) LLMs are already used to aid with judging LLM outputs and this approach is more scalable than relying on humans alone (Bai et al., 2022; Saunders et al., 2022; Markov et al., 2023), (2) varying the capability gap (in terms of pretraining compute) between prover and verifier lets us study legibility as a function of this gap, and (3) when human feedback is limited for advanced systems, making outputs verifiable by less capable verifiers may incrementally aid human legibility (Leike et al., 2018; Christiano et al., 2018).
\*Equal contribution, order decided by coin flip. Project conducted by the Superalignment Games team.
†Work done while at OpenAI.
![](81bf7a5187d196cc72844283e3aeec49_img.jpg)
grade school math performance
Figure 1 is a scatter plot showing accuracy (human evaluator) on the y-axis (ranging from 0.65 to 0.85) versus accuracy (model generator) on the x-axis (ranging from 0.3 to 0.9). The plot contains three key data points:
| Label                    | Accuracy (model generator) [x] | Accuracy (human evaluator) [y] |
|--------------------------|--------------------------------|--------------------------------|
| init.                    | ~0.4                           | ~0.82                          |
| checkability game (ours) | ~0.62                          | ~0.82                          |
| RL                       | ~0.85                          | ~0.72                          |
Visual annotations include:
- A horizontal black arrow pointing from 'init.' to 'checkability game (ours)'.
- A solid black line connecting 'init.' and 'RL'.
- A vertical dashed line dropping from 'checkability game (ours)' to the solid black line.
- A red horizontal bar labeled "legibility tax RL" at y ~ 0.72, extending from x ~ 0.62 to x ~ 0.85.
Figure 1: **Key result.** Optimizing for solution correctness leads to high accuracy, but poor legibility. Initialization (pretrained on a variety of human-written math derivations) has higher legibility, but worse accuracy. Our proposed method retains high legibility with an accuracy in between the initialization and the correctness-only baseline. The resulting trade-off illustrates a **legibility tax**.
Checkability — i.e. the capacity of a verifier to independently check the correctness of solutions generated by a *helpful* prover — can be defined by a combination of two properties:
1. **Completeness.** For any input in the domain of interest, the helpful prover generates correct solutions and the verifier accepts them as correct.
2. **Soundness.** The verifier rejects all incorrect solutions produced by any adversarial provers of the same model class as the helpful one.
The definitions are inspired by literature on Probably-Approximately Correct (PAC) verification and interactive proof systems (Goldwasser et al., 2021; Mutreja and Shafer, 2023). In informal domains, neither models nor humans can be expected to be perfectly complete or sound. We instead study relaxed versions of completeness and soundness. Instead of perfect **completeness** we expect the helpful prover to have high pass rate and the verifier to have high recall on helpful samples. Instead of perfect **soundness** we expect the verifier to have low false positive rate on adversarial inputs produced through limited amounts of optimization. When we find helpful provers and verifiers with such limited completeness and soundness properties, we say that the helpful prover is legible to the verifier. This is analogous to saying that outputs are legible to humans if humans accept them as correct and can’t be fooled by similar-looking incorrect outputs.
To obtain legible provers and robust verifiers, we propose using checkability training - an iterative algorithm inspired by the Prover-Verifier Game (PVG; Anil et al. 2021). We formulate a version of the game where the verifier moves first assuming that the prover will play best response strategy conditioned on the verifier’s strategy (i.e., the verifier "leads" and the prover "follows"), and show that for provers with unconstrained model class, finding a verifier-leading Stackelberg equilibrium is equivalent to finding a perfectly legible protocol with respect to the verifier (Theorem 1). Our iterative algorithm is inspired by that version of the game, while optimizing losses that have better convergence properties in practice.
In practice we alternate between training a verifier language model and a prover language model that we prompt to act either helpful or sneaky on disjoint subsets of grade-school math problems with known ground truth answers (Cobbe et al., 2021)<sup>1</sup>. In each round, we first train the verifier using supervised learning to predict correctness of solutions sampled from a balanced mixture of provers from previous rounds. Next, we optimize the prover using reinforcement learning with respect to a reward combining the solutions’ correctness and the current round verifier acceptance
<sup>1</sup>We focus on the easier case where some ground truth labels are available for improving the players, and leave the semi-supervised or unsupervised settings for future work (see Section 6.1 for more discussions).
![](ad04aa6cb9e7fb36bb7a91e817e2d314_img.jpg)
| init                                                                                                                                                                                                   | round 1 of checkability training                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | round 5 of checkability training                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Shawna's father is five times as old as Shawna. Shawna is currently three times as old as Aliya. If Aliya is 3 years old, how old is Shawna's father?                                                  | Shawna's father is five times as old as Shawna. Shawna is currently three times as old as Aliya. If Aliya is 3 years old, how old is Shawna's father?                                                                                                                                                                                                                                                                                                                                                                                                                         | Shawna's father is five times as old as Shawna. Shawna is currently three times as old as Aliya. If Aliya is 3 years old, how old is Shawna's father?                                                                                                                                                                                                                                                                                                                                                                      |
| <p>It follows that Shawna is currently <math>3^2 = 9</math> years old.</p> <p>Subsequently this implies that Shawna's father is currently <math>9 \cdot 5 = 45</math> years old.</p> <p>Answer: 45</p> | <p>First we need to determine the age of Aliya. From the problem description, it says that Aliya is 3 years old.</p> <p>Then we need to determine the age of Shawna. From the problem description, we know that Shawna is currently three times as old as Aliya, so the age of Shawna is 3 times 3, or &lt;&lt;3 * 3 = 9&gt;&gt;9.</p> <p>Next, we need to determine Shawna's father. The problem states that Shawna's father is five times as old as Shawna. Therefore, the age of Shawna's father becomes 5 times 9, or &lt;&lt;5 * 9 = 45&gt;&gt;45.</p> <p>Answer: 45</p> | <p>We start by calculating the age of Aliya.</p> <p>It says that Aliya is 3 years old.</p> <p>Then, we need to calculate Shawna's current age.</p> <p>Shawna is currently three times as old as Aliya. Thus, Shawna's current age is 3*3 = &lt;&lt;3*3=9&gt;&gt;9.</p> <p>Now we need to calculate Shawna's father's age.</p> <p>Shawna's father is five times as old as Shawna. Therefore, Shawna's father's age is 5 * 9 = &lt;&lt;5*9=45&gt;&gt;45.</p> <p>Therefore Shawna's father's age is 45.</p> <p>Answer: 45</p> |
Figure 2: **Checkability training produces legible and well structured solutions.** Three sample solutions produced by the helpful prover from different rounds of checkability training.
rate ("convincingness"). The prover is conditioned on a role sampled uniformly from "helpful" and "sneaky". In the helpful role, we reward being convincing and correct. In the sneaky role, we reward being convincing and incorrect.
Our main results are illustrated in Figure 1:
- **Direct reinforcement learning on solution correctness produces solutions that are difficult for humans to evaluate.** If we optimize our prover only for solution correctness, it can achieve high performance on grade-school math problems. However, the solutions produced by the model are very different from human-written solutions and consequently are difficult to evaluate for time-constrained humans (Section 4.1).
- **Our proposed checkability training achieves decent performance while retaining legibility.** After checkability training, we obtain a a prover that produces solutions that have better performance-legibility trade-off. It has lower test performance than the prover optimized for correctness alone, which represents an instance of a *legibility tax* (Leike, 2022). The checkability training partially closes the gap, as the prover's pass rate improves in later rounds of training (Section 4.3.3).
Verifiability training produces three useful artifacts — a robust verifier, a sneaky prover, and a helpful prover. We evaluate the legibility of the sneaky and helpful prover as well as the soundness of the verifiers. The main findings are:
- The verifiers show increasing soundness over training rounds, as measured via difficulty of various sneaky-only attacks and balanced best-of- $n$  accuracy (Section 4.3.1).
- The sneaky prover generates increasingly subtle flaws, since the average step-by-step correctness judged by GPT-4 increases, and the average human judge accuracy on sneaky solutions decreases (Section 4.3.2).
- The helpful prover achieves higher pass rates, and human trainers judge them correctly more often at high rounds (Section 4.3.3).
Overall, we found it surprising that legibility to smaller LLMs seems to transfer to legibility to humans — sneaky solutions that are harder to detect for smaller verifiers are also harder to detect for humans, and helpful solutions that are more legible to smaller verifiers are also more legible to humans. In the case that future LLMs acquire superhuman capabilities, training them to be more legible will help humans evaluate their correctness. Hence, prover-verifier game is a promising candidate for a scalable oversight method.