# PROVER-VERIFIER GAMES IMPROVE LEGIBILITY OF LLM OUTPUTS
Jan Hendrik Kirchner\* Yining Chen\*
Harri Edwards† Jan Leike† Nat McAleese Yuri Burda†
OpenAI

## 3 SETUP
In general, we consider a dataset of problems with ground truth answers  $(x,y) \in \mathcal{X} \times \mathcal{Y}$ , where  $x$  denotes a problem statement and  $y = y(x)$  denotes the known and unique ground truth answer. Given a proposed solution  $z$  to the problem  $x$  including a final answer, we let  $\mathbb{1}_{\text{correct}}(x,z)$  be the correctness indicator that is 1 if the solution proposes an answer that is the same as  $y(x)$  and 0 otherwise. We use a SymPy grader (Meurer et al., 2017) for evaluating  $\mathbb{1}_{\text{correct}}(x,z)$  on grade-school math (GSM) problems.
We have a prover  $\pi(z|x)$  proposing solutions given a problem statement, and a verifier  $v(x,z) \in [0, 1]$  that estimates the correctness of the solution's answer. We call the output of the verifier the *convincingness score*.
### 3.1 FROM CHECKABILITY TO THE PROVER-VERIFIER GAME
We start with a formal definition of checkability. Prover  $\pi$  from model family  $\mathcal{H}$  is *verifiable* to verifier  $v$  on the data distribution  $P_{XY}$  if and only if the prover always generates *correct* and convincing solutions, and no adversarial / "sneaky" prover from the same model family  $\mathcal{H}$  can fool the verifier into accepting incorrect solutions. Formally,
- Completeness:  $\forall x \in \mathcal{X}, \mathbb{1}_{\text{correct}}(x, \pi(x)) = v(x, \pi(x)) = 1$ .
- Soundness:  $\forall \pi' \in \mathcal{H}, \forall x \in \mathcal{X}, \mathbb{1}_{\text{correct}}(x, \pi'(x)) = 0 \Longrightarrow v(x, \pi'(x)) = 0$ .
A few notes on the definition above: First, checkability doesn't necessarily capture everything we intuitively want from legibility, and we hope to inspire other definitions in future works. Second, we restricted the sneaky prover to be from the same model family as the helpful one. This is because we are interested in answering the question "Can one intelligent system trust another intelligent system of a certain size, architecture, and trained with certain algorithms?". Hence we only have to ensure that the verifier is robust to the class of models we solicit answers from. Third, in empirical work we only seek to find approximate completeness and soundness. Since the verifiers and provers we study are neural networks, it is impractical to find perfect completeness and soundness. Therefore we study a relaxation of completeness and soundness in the following sense: (1) for the helpful prover we require that it is correct in distribution  $P_{XY}$  with high probability; and (2) for the verifier, we require that it is robust to a limited set of black-box attacks that are relatively "close" to the honest prover (intuitively, we want the verifier not to miss small errors that would make the conclusion invalid). We will compare verifiers' soundness via their ranking on a variety of attacks, and the more comprehensive the attacks are the closer we get to measuring soundness.
We propose to train legible prover and sound verifier using variants of the prover-verifier game (Anil et al., 2021). We formulate it as a two-player game between a role-conditioned prover  $\pi$  and the verifier  $v$ . We use shorthand  $h(x) = \pi(x, \text{role}=\text{helpful})$  for the prover conditioned on the helpful role and  $s(x) = \pi(x, \text{role}=\text{sneaky})$  for the prover conditioned on the sneaky role.
We define the verifier's utility as a negative classification loss on a uniform mixture between the helpful and the sneaky samples,
$$R_V = -\frac{1}{2}\mathbb{E}_{x \sim P_X} l(v(x, h(x)), \mathbb{1}_{\text{correct}}(x, h(x))) - \frac{1}{2}\mathbb{E}_{x \sim P_X} l(v(x, s(x)), \mathbb{1}_{\text{correct}}(x, s(x)))$$
where  $l(v, c) : [0, 1] \times \{0, 1\} \rightarrow \mathbb{R}$  is a loss function minimized if and only if  $v = c$ , such as the cross-entropy loss. We assume without loss of generality that  $l(c, c) = 0$ . In practice we optimize  $R_V$  via gradient descent on  $l_V = -R_V$ .
The policy is conditioned on either the helpful or the sneaky role with equal probability, and the combined reward is
$$R_P = \frac{1}{2}\mathbb{E}_{x \sim P_X} r(v(x, h(x)), \mathbb{1}_{\text{correct}}(x, h(x))) + \frac{1}{2}\mathbb{E}_{x \sim P_X} r(v(x, s(x)), 1 - \mathbb{1}_{\text{correct}}(x, s(x)))$$
where  $r(v, q) : [0, 1] \times \{0, 1\} \rightarrow \mathbb{R}$  is a reward function that satisfies:
$$\forall v_1, v_2 \in [0, 1] : r(v_1, 1) > r(v_2, 0),$$
$$\forall v_1, v_2 \in [0, 1], v_1 > v_2 : r(v_1, 1) > r(v_2, 1). \tag{1}$$
Here  $q \in \{0, 1\}$  indicates that the correctness of the solution is role-aligned — it is 1 if the prover is assigned to be helpful and the solution is correct, or the prover is assigned to be sneaky and the solution is incorrect. The two assumptions on the prover reward functions above are: First, any role-aligned solution gets higher reward than any role-misaligned solution; Second, for any role-aligned solution, the prover gets higher reward when the verifier gives the solution higher score. In practice we optimize  $R_P$  via reinforcement learning since we cannot differentiate with respect to the solution, because it requires iterative auto-regressive sampling of LLM tokens, a non-differentiable operation.
We show a connection between the prover-verifier game and the completeness and soundness objectives as follows:
**Theorem 1.** *Assuming that  $\mathcal{H}$  is the set of all deterministic mappings from  $\mathcal{X}$  to  $\mathcal{Z}$ , in the PVG where verifier’s utility is  $R_V$  and the prover’s utility is  $R_P$ , the existence of a verifier-leading Stackelberg equilibrium  $(v^*, h^*, s^*)$  is necessary and sufficient for the completeness and soundness properties to hold for  $(v^*, \pi^*)$ .*
One of the players (in our case, the verifier) is the leader, and the other player (the prover) is the follower. We assume that given a leader strategy, the follower will always play its best response, and the leader picks its strategy based on this knowledge. A Stackelberg equilibrium means that none of the players have incentives to change its strategy. The proof is in Appendix A. Compared to Theorem 2 in Anil et al. (2021), we have more relaxed assumptions on the loss function and reward.
In practice, LLMs are probabilistic models and the hypothesis class of LLMs is only a subset of all mappings from  $\mathcal{X}$  to  $\mathcal{Z}$ , so our empirical setup does not match the theoretical assumptions exactly. Furthermore, when the players are nonconvex models like neural networks, we do not have formal guarantees for finding a Stackelberg equilibrium efficiently. However, we can train the models using coordinate descent to find an approximate local equilibrium.
### 3.2 CHECKABILITY TRAINING
In practice, we design training objectives for the verifier and the provers using slight variations on  $l$  and  $r$ . Most notably, we train the verifier using a relative loss based on comparisons between correct and incorrect solutions for each question (Bradley and Terry, 1952), instead of an absolute loss for each solution. For the prover reward  $r$ , we have three variants that prioritizes verifier score  $v$  and role-alignment  $q$  differently. Both provers and verifiers are from the GPT4 family (Achiam et al., 2023). We initialize the prover and the verifier with different sized models that have been lightly finetuned on a variety of math data from the pretaining distribution, including human written proofs and derivations. We discuss the algorithmic details below.
First, we augmented the GSM (Cobbe et al., 2021) dataset using 100k synthetically generated and validated datapoints from ChatGPT, similar to the method in Liu et al. (2023). We made the dataset larger so as not to be bottlenecked by sample efficiency in order to focus on the training dynamics. We validated that the original test set accuracy is not impacted by using real vs. synthetic data. Next, we randomly partitioned the training dataset  $\mathcal{D}$  into two equally-sized subsets  $\mathcal{D}_\pi$  and  $\mathcal{D}_V$  that are used for training the prover  $\pi$  and the verifier  $V$  respectively. This way the verifier and the prover are never optimized on the same prompt.
The training proceeds in multiple rounds, and each round the **verifier training** phase precedes the **prover training** phase. The only change between rounds is the data mixture used to train the verifier; no other state, such as model weights, is carried over between rounds.
In the **verifier training** phase of round  $i$  we train verifier  $V_i$  to minimize the regularized comparison loss
$$\mathbb{E}_{x\sim\mathcal{D}_V}\left[\mathbb{E}_{z_1,z_2\sim\pi^*}\log\sigma\left[(V(x,z_1)-V(x,z_2))\cdot\text{sign}(\mathbb{1}_{\text{correct}}(z_1)-\mathbb{1}_{\text{correct}}(z_2))\right]+\lambda E_{z\sim\pi^*}V(x,z)^2\right]$$
Here  $V(x,z)\in\mathbb{R}$  is a scoring function and not restricted to range  $[0, 1]$  like the theoretical formulation. We use capitalized  $V$  to avoid confusion. The first term is the reward modeling objective that maximizes log likelihood of correct solutions being ranked higher than incorrect solutions under the Bradley-Terry model, similar to Ouyang et al. (2022). The second term is a regularizer that is meant to center  $V$  at 0. Without this term, adding an arbitrary per-problem constant to  $V$  would not change the objective. In preliminary experiments we found this objective to be interchangeable to the cross-entropy loss for the overall dynamics of checkability training and we adopted this form due to engineering convenience.
The prover  $\pi^*$  used for sampling solutions for verifier training is a mixture of an initial base prover and previous round provers, each balanced to have equal number of correct and incorrect solutions. Each of the previous rounds has equal ratio in the mixture, which is tuned as a hyperparameter. In round 0, the solutions are from the base prover sampled via a few-shot prompt (App. H). In later rounds of training, we replace part of the solutions with those from the new provers. Hence we always use the same number of solutions per problem. The amount of verifier optimization is constant over the rounds.
During the **prover training** phase we use Proximal Policy Optimization (PPO; Schulman et al. 2017) to maximize an expected reward  $R$  derived from the current round verifier score and whether the correctness is aligned with the assigned role. The prover’s objective is to maximize the following reward (minus a small KL regularization towards the initialization):
$$\mathbb{E}_{x\sim D_\pi, \text{role}\sim\{\text{helpful, sneaky}\}, z\sim\pi(\cdot|x, \text{role})} R(z \mid x, \text{role}).$$
We experimented on three variants of  $R$ . We call the first reward variant the Signed Relative Convincingness (SRC) reward. Given a problem  $x$ , let the role indicator  $\mathbb{1}_{\text{helpful}}(\text{role})$  be 1 if the prover is assigned to be helpful and 0 otherwise. Suppose conditioned on a role and  $x$ , the prover generated a batch of solutions  $\{z_0, \dots, z_b\} \sim \pi(\cdot \mid x, \text{role})$  with verifier scores  $\{V(x, z_0), \dots, V(x, z_b)\}$ . Since the scores are not normalized, we first batch-normalize the scores to mean 0, i.e.  $V'(x, z_i) = V(x, z_i) - (\sum_{j=1}^b V(x, z_j))/b$ , and then set the reward as follows:
$$R_{\text{SRC}}(z_i \mid x, \text{role}) = \begin{cases} (2\mathbb{1}_{\text{correct}}(x, z_i) - 1) \cdot (2\mathbb{1}_{\text{helpful}}(\text{role}) - 1) \cdot V'(x, z_i) & \text{if } V'(x, z_i) > 0 \\ -1 & \text{otherwise} \end{cases} \quad (2)$$
Ignoring the verifier score range discrepancy, the first case is equivalent to setting  $r(v, q) = v \cdot (2q - 1)$ , i.e. when  $\mathbb{1}_{\text{correct}}(x, z_i) = \mathbb{1}_{\text{helpful}}(\text{role})$ , the reward equals the normalized verifier score, otherwise it equals negative score. This case satisfies our assumptions (1) that any role-aligned solution has higher score than any non-aligned one, and for any two role-aligned solutions, the one with higher verifier score gets higher reward. However, the second case where any solution below average score is directly assigned a score of -1 deviates from the theoretical formulation. The rationale for this case is to prioritize convincingness over role-alignment. We show in section 4.2 that this is practically useful for convergence.
The second reward variant that prioritizes role-alignment is the Correctness-Gated Convincingness (CGC) reward:
$$R_{\text{CGC}}(z \mid x, \text{role}) = \begin{cases} V(x, z) & \text{if } \mathbb{1}_{\text{helpful}}(\text{role}) = \mathbb{1}_{\text{correct}}(x, z) \\ V_0 & \text{otherwise} \end{cases} \quad (3)$$
where  $V_0$  is a constant that is practically lower than all verifier scores (we used -2 in our experiments). This reward corresponds to  $r(v, q) = qv + (1 - q) \inf v$  which satisfies the reward assumptions (1) ignoring the discrepancy in verifier score range. Empirically, this reward leads to worse dynamics than the SRC reward (Section 5.1).
We also consider a setup that we call **Iterative Goodharting** where the helpful and sneaky provers both use the following reward,
$$R_{\text{goodhart}}(z \mid x) = V(x, z) \quad (4)$$
which means the reward encourages only convincingness, i.e.  $r(v, q) = v$ . This version does not satisfy assumptions (1) because  $r(v, 0) = r(v, 1)$ . The empirical results are in Section 5.2.