# PROVER-VERIFIER GAMES IMPROVE LEGIBILITY OF LLM OUTPUTS
Jan Hendrik Kirchner\* Yining Chen\*
Harri Edwards† Jan Leike† Nat McAleese Yuri Burda†
OpenAI

## 6 DISCUSSION
### 6.1 LIMITATIONS
**Only domains that allow natural language justifications.** We studied the math domain because natural language explanations are common and often required for building trust in mathematical conclusions. However, in some domains it is difficult to support conclusions with natural language explanations. For example recognizing particular faces is easy for humans, but it’s hard to communicate to others in natural language why a particular face belongs to a particular person.
**Need for ground truth answers and reliance on generalization.** One prerequisite for our setup is being able to certify correctness of answers of solutions produced by the prover. For the grade school math domain we rely on having a training dataset of problems with known ground truth answers (Cobbe et al., 2021). We anticipate this setup to be less relevant to domains where collecting any ground truth is too hard or expensive. One possible approach in this case could be to hope for favorable out-of-distribution generalization properties of legibility from solutions to problems with known ground truth answers to other domains. We don’t study out-of-domain generalization in this paper because we train on only one dataset, but we expect that training on a diverse set of domains
could potentially lead to good generalization from domains with ground truth labels to the domains without labels (Burns et al., 2023).
**Conflating the model’s chain-of-thought with legible solutions.** By training the CoTs to be legible, we potentially limited the provers’ accuracy. A better method could be training a first model using CoT to maximize accuracy, and then training a second model to turn the CoT used to produce the answer into a legible justification to humans. Hence, our measurement of accuracy-legibility tradeoff may be overly pessimistic; the prover may be able to achieve legibility at less or no cost to accuracy. For the correctness-only baseline, we could have early stopped at less accurate but more legible checkpoint. However, we don’t know of a principled way to do early stopping for legibility.
**Dataset diversity and difficulty.** We conducted our empirical study on only one dataset (GSM). The dataset is relatively easy and the solutions are quite standardized. Using ChatGPT to augment this dataset could result in some incorrect labels, but based on preliminary data analysis we did not see significant difference between verifier test accuracies trained on synthetic vs. real data. We would like to extend the checkability training framework to harder math datasets and coding, as well as fuzzier domains such as human preference data (Ouyang et al., 2022). Our method is fully general as long as there is a finite set of final answer labels.
**Initialization with human-written math derivations.** Our prover initialization has relatively high performance in GSM. We used a prover that has been lightly finetuned on a variety of math data to learn the correct format, and the finetuning dataset contains some of the GSM trainset data. We believe this doesn’t qualitatively affect our results and only makes it easier for the helpful prover to produce correct answers. Figure S9 shows that initialization with a base model without math finetuning leads to qualitatively similar training dynamics.
**Heterogeneous experiment set-ups.** We have different stopping criteria for different training schemes presented throughout the paper. We trained the prover for a fixed number of RL steps in each round for certain training schemes (4k steps in Fig. 6a, 5c, 8k steps in Fig. 4, 5c, S4, S5), whereas in Fig. 5, 9, 10 we trained until early stopping criteria are met (the criteria are already discussed in those section). We do not observe qualitative different between early stopping and non early stopping rounds in our experiments.
**Hyperparameters.** We did not tune all hyperparameters thoroughly for all training schemes in this paper. The main hyperparameter we tuned are the ratios of samples from each of the previous rounds. We tuned the ratio for one verifier size ( $10^{-3}$ ) for the SRC reward based on whether training robustness increases across rounds, and used the same ratio for other sizes with SRC and CGC rewards, which could result in other training schemes being worse in robustness and legibility. We separately tuned the ratio for one verifier size ( $10^{-3}$ ) for the iterative goodharting training. We used the default ChatGPT post-training hyperparameters such as the optimizer, learning rate, or KL regularization coefficient for prover and verifier training.
### 6.2 FUTURE WORK
We studied the setting in which correctness labels are available at training time for all problems. A more realistic scenario is one where not all answers have a trusted final correctness label, in which case the verifier would rely on unsupervised signals such as consistency (Wang et al., 2023) and convincingness (Khan et al., 2024). Unsupervised methods for improving legibility are an important future direction.
Another direction is developing a method to turn an illegible solution into a legible one. The CoTs that the model first uses to solve a problem are not legible to a smaller model per se but we could train a translation model that turns the ‘scratch space’ CoT to a well-written proof. This approach might be favorable over directly training the CoT to be legible since exerting any optimization pressure on the CoT may limit prover performance as well as exacerbate deceptive alignment (i.e., the model hiding its own intent) (Denison et al., 2024; Hubinger et al., 2024).
The training schemes we consider are adaptations of single-turn simultaneous games. With an interactive tool such as ChatGPT, the verifier can interact with the prover in more rounds which
should increase the sample efficiency vastly based on complexity theory (Goldwasser et al., 2021). An example of multi-turn prover game is a “mind-changing” game: for binary outcome predictions, the prover maximally tries to convince the verifier that the answer is true / false alternatingly in context. This is similar to the market-making proposal (Hubinger, 2020).
We mainly studied legibility to smaller models in this paper. However, legibility to humans is what we eventually care about, so future work should more systematically study what type of assistance better help humans judge the validity of large model solutions. We expect more extensive human study to be fruitful.
### 6.3 CONCLUSION
We presented a tension between model performance and solution legibility in a concrete setting of judging grade-school math solutions, and proposed one way to address the tradeoff. Our results show promising signs for establishing trust in LLM outputs even if they become more capable than humans in the future. We hope to inspire future work on semi-supervised / unsupervised setups for improving human judge legibility with few or no ground truth labels.