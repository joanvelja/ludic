# PROVER-VERIFIER GAMES IMPROVE LEGIBILITY OF LLM OUTPUTS
Jan Hendrik Kirchner\* Yining Chen\*
Harri Edwards† Jan Leike† Nat McAleese Yuri Burda†
OpenAI

## 2 RELATED WORK
**Computational complexity.** Our training setup is most similar to learning a verification protocol for NP (Micale, 2000). If we allow more interactions, we can extend the PVG setup to interactive proofs (Babai, 1985; Goldwasser et al., 2019). Goldwasser et al. (2021) and Mutreja and Shafer (2023) applied interactive proofs to Probabilistic-Approximately Correct (PAC) verification. They showed that a verifier can learn a hypothesis class with much less data by interacting with an untrusted prover ( $O(\sqrt{VC(\mathcal{H})})$  sample complexity for a hypothesis class of Vapnik-Chervonenkis dimension  $VC(\mathcal{H})$ ). In our work, we assume capability (instead of data) constraint on the verifier (it cannot independently produce a solution without the help of a prover even with infinite data). Anil et al. (2021) first made the connection from prover-verifier games to the alignment problem. They proved that different game setups lead to equilibria of different desirability, and empirically showed promise in toy experimental settings. Wäldchen et al. (2024) applied prover-verifier games to interpretability for image classification. Unlike all those previous works, our verifier’s task is not classifying the inputs but rather judging the correctness of solutions. Amit et al. (2024) trained LLM provers against a manually-defined verifier; in contrast, our verifier is learned. We take inspirations from those theoretical works and aim to empirically train for checkability on a nontrivial domain at scale.
**Scalable oversight.** The central hypothesis that discrimination is easier than generation is common to many scalable oversight methods (Bowman et al., 2022) including critiques (Saunders et al., 2022; McAleese et al., 2024) and debate (Irving et al., 2018). Parrish et al. (2022) showed negative results for improving time-limited human judge accuracy though human debate. Previous works have also studied using LLMs as judges or debaters, although they differ on whether ground truth labels are available for training. Our setup is similar to single-turn simultaneous debate where the helpful and sneaky provers try to convince the model judge of different answers, assuming that some ground truth labels are available for improving both the provers and the verifier. Similarly, using supervised training, Radhakrishnan (2023) trained model debaters and judges and showed that the optimization makes the debaters more convincing and the judges more accurate. Michael et al. (2023) showed that debate works better than consultancy (showing the argument from a helpful prover half the time and sneaky half the time) for human judges. In our setup the judge does not see both sides at the same time, so it is more similar to the consultancy approach. Khan et al. (2024) showed that LLMs trained using persuasiveness signal only (no ground truth labels) can help both non-expert model and human judges. All of those works mainly use the QuALITY dataset (Pang et al., 2022), where the task is question-answering about a fictional story, so the justifications mainly consist of verified quotes from the text. In contrast, we focus on a less artificial, more challenging reasoning domain where the difference between provers and verifiers isn’t privileged information but capability gap.
**Chain-of-Thought.** Prompting LLMs to produce Chain-of-Thought (CoT) solutions is useful for boosting accuracy (Wei et al., 2022) and CoTs can be used as natural justifications for the final answer. However, CoTs are not guaranteed to be legible a priori and our checkability training is one way to train CoTs to be more legible. Many works guide the model to produce a specific type of structured CoT, such as task decomposition (Christiano et al., 2018; Radhakrishnan et al., 2023; Wen et al., 2024), recursive summarization (Wu et al., 2021), and symbolic reasoning (Lyu et al., 2023). Lightman et al. (2024) use expensive step-wise human supervision to train models that can solve math. In contrast, we do not enforce any output structure, and rely solely on final answer supervision. Our checkability training method is general and can be readily studied in any domain where we have some ground truth labels. Legibility emerges as a natural consequence of the training dynamics (Figure 2).
**Explainability and legibility as complementary properties.** Explainability (Zhao et al., 2024) tries to give an account of the prover model’s reasoning, whether the reasoning is correct or not. The resulting explanations can be arbitrarily complex (Nanda et al., 2023). In contrast, legibility of a prover model’s output allows to verify if a given solution is correct, ignoring how the prover reasoned about it. Consequently, legibility requires model outputs that are coherent and accessible to human understanding (Rudin, 2019). It is possible to have either without the other, but especially in high-stakes settings correctness is more important than understanding (Ziegler et al., 2022).
![A schematic illustrating the legibility problem. A human asks a superhuman AI to 'Write me a fast sorting algorithm'. The AI responds with a complex, illegible block of code. The human reacts with '???' and labels identify them as 'Human' and 'Superhuman AI'.](bb08c83fc8939517c6803d65c69dd06b_img.jpg)
The figure shows a Human asking a Superhuman AI to "Write me a fast sorting algorithm". The AI responds with a block of code that is illegible to the human, who reacts with "???".
```
for (k = 2; k <= n; k *= 2)
  for (j = k/2; j > 0; j /= 2)
    for (i = 0; i < n; i++)
      l = bitwiseXOR (i, j);
      if (l > i)
        if ( (bitwiseAND (i, k) == 0) AND (arr[i] > arr[l])
          OR (bitwiseAND (i, k) != 0) AND (arr[i] < arr[l]) )
          swap the elements arr[i] and arr[l]
```
A schematic illustrating the legibility problem. A human asks a superhuman AI to 'Write me a fast sorting algorithm'. The AI responds with a complex, illegible block of code. The human reacts with '???' and labels identify them as 'Human' and 'Superhuman AI'.
Figure 3: **Schematic illustrating the legibility problem.** A poorly aligned, superhumanly capable AI might produce highly efficient code that sacrifices legibility for conciseness or performance. Code is slightly adapted from the bitonic sorter for parallelized sorting of elements in a list (Wikipedia contributors, 2023).
**Adversarial Robustness.** Deep neural networks are susceptible to adversarial attacks (Szegedy et al., 2013; Biggio et al., 2013). The decade-long work on adversarial defenses had limited success for vision models (Croce et al., 2020). Language models have discrete input-output tokens and hence its not as straight-forward to perform gradient-based attacks (Carlini et al., 2024) but white-box (Zou et al., 2023) and black-box (Shah et al., 2023; Hayase et al., 2024) jailbreaks that can undo the alignment process are still found. Requiring a perfectly robust verifier is not realistic either for models or humans (Elsayed et al., 2018). Our checkability training can be viewed as adversarial training on the verifier, and we show that robustness transfers to test attacks different from training attacks in Section 4.3.1, which is similar to common methods in the adversarial robustness literature (Carlini et al., 2019).
**Legibility in concrete applications.** Prior works have studied legibility in different contexts. For example, in robotics, legibility refers to being able to interpret an agent’s intent based on its action (Dragan et al., 2013). For coding, human legibility has multiple facets including being able to correctly trace the execution, find bugs, or summarize goals (Oliveira et al., 2020; Wen et al., 2024). In this work we measured human legibility via the accuracy of their judgement of solution correctness in time-limited settings, which is only one of many ways to measure human legibility.
