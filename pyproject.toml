[project]
name = "ludic"
authors = [
    {name = "hallerite", email = "me@hallerite.com"},
]
description = "Ludic - The LLM-RL library for the age of experience"
version = "0.1.0"
readme = "README.md"
requires-python = "~=3.12.0"
dependencies = [
    "aiohttp>=3.13.2",
    "beartype>=0.22.9",
    "jaxtyping>=0.3.4",
    "datasets>=4.4.2",
    "openai>=2.7.1",
    "peft>=0.18.0",
    "rich>=14.2.0",
    "setuptools>=79.0.1",
    "torchmetrics>=1.0.0",
    "transformers>=4.52.0",
    # CRITICAL: torch>=2.9.0 required for aarch64 CUDA wheels
    # PyTorch 2.8.0 has NO aarch64 CUDA wheels - skip it!
    # See: https://download.pytorch.org/whl/cu128/torch/
    "torch>=2.9.0",
    # vLLM is Linux-only (depends on NVIDIA libraries)
    # Use sys_platform marker to skip on macOS/Windows
    "torch-c-dlpack-ext>=0.1.4",
    "vllm>=0.12.0; sys_platform == 'linux'",
    "wandb>=0.23.1",
]

[project.optional-dependencies]
code-exec = [
    "docker>=7.1.0",
]

[build-system]
requires = ["uv_build>=0.9.5,<0.10.0"]
build-backend = "uv_build"

[dependency-groups]
test = [
    "pytest>=8.4.2",
    "pytest-asyncio>=1.2.0",
]
lint = [
    "ruff>=0.14.5",
]
typing = [
    "ty>=0.0.5",
]
dev = [
    { include-group = "test" },
    { include-group = "lint" },
    { include-group = "typing" },
]

[tool.pytest.ini_options]
addopts = ["-m", "not diagnostic"]
markers = [
    "integration: marks tests as integration tests",
    "gpu: marks tests as needing a GPU",
    "diagnostic: marks tests that primarily emit diagnostic reports rather than asserting strict correctness",
]
testpaths = ["tests"]

# =============================================================================
# uv Configuration for Cross-Platform PyTorch
# =============================================================================
# This configuration automatically selects the correct PyTorch wheels:
#   - Linux: CUDA 12.8 wheels from pytorch-cu128 index
#   - macOS/Windows: CPU wheels from pytorch-cpu index
#
# Usage:
#   Local dev (macOS):  uv sync
#   HPC (Linux GPU):    uv sync
#   Linux CI (no GPU):  uv sync --extra cpu
#
# See: https://docs.astral.sh/uv/guides/integration/pytorch/
# See: https://docs.isambard.ac.uk/user-documentation/applications/ML-packages/
# =============================================================================

[tool.uv]

# Platform-based torch source selection:
# - Linux: Use CUDA 12.8 wheels (supports both x86_64 and aarch64)
# - Non-Linux (macOS, Windows): Use CPU wheels
[tool.uv.sources]
torch = [
    { index = "pytorch-cpu", marker = "sys_platform != 'linux'" },
    { index = "pytorch-cu128", marker = "sys_platform == 'linux'" },
]
torchvision = [
    { index = "pytorch-cpu", marker = "sys_platform != 'linux'" },
    { index = "pytorch-cu128", marker = "sys_platform == 'linux'" },
]

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true
