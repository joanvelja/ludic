"""
Eval a vLLM-served model on Tic-Tac-Toe.

Example:
    uv run python examples/tic_tac_toe/eval_tic_tac_toe_vllm.py \
        --model Qwen/Qwen2.5-7B-Instruct \
        --host 127.0.0.1 --port 8000 \
        --episodes 200
"""

from __future__ import annotations

import argparse
from typing import Dict, List

from ludic.inference import VLLMChatClient
from ludic.parsers import compose_parsers, think_prefix_parser, xml_tag_parser
from ludic.eval.core import run_eval_sync
from ludic.training import (
    EnvSpec,
    ProtocolSpec,
    Reducer,
    RolloutRequest,
)
from environments.tic_tac_toe import TicTacToeEnv
from ludic.eval.cli import (
    add_common_eval_args,
    build_single_agent_engine,
    maybe_start_vllm,
    sampling_args_from_cli,
    write_jsonl,
)


TICTACTOE_REDUCERS: Dict[str, Reducer] = {
    "win_rate": Reducer(kind="count_true", source="result", transform=lambda v: v == "win", normalize_by="rollouts", as_percent=True),
    "loss_rate": Reducer(kind="count_true", source="result", transform=lambda v: v == "loss", normalize_by="rollouts", as_percent=True),
    "draw_rate": Reducer(kind="count_true", source="result", transform=lambda v: v == "draw", normalize_by="rollouts", as_percent=True),
    "illegal_rate": Reducer(kind="count_true", source="illegal_move", normalize_by="rollouts", as_percent=True),
    "parse_error_rate": Reducer(kind="count_true", source="parse_error", normalize_by="rollouts", as_percent=True),
    "avg_completion_tokens": Reducer(kind="mean", source="completion_length"),
}


def make_requests(episodes: int, args: argparse.Namespace) -> List[RolloutRequest]:
    sargs = sampling_args_from_cli(args)
    return [
        RolloutRequest(
            env=EnvSpec(kind="tictactoe", kwargs={"agent_starts": True}),
            protocol=ProtocolSpec(kind="single_agent"),
            seed=seed,
            sampling_args=sargs,
            num_episodes=1,
            meta={"seed": seed},
        )
        for seed in range(episodes)
    ]


def make_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Evaluate a model on Tic-Tac-Toe.")
    add_common_eval_args(parser)
    parser.add_argument("--episodes", type=int, default=200, help="Number of episodes.")
    parser.set_defaults(temperature=0.6, max_tokens=250, max_steps=5, out="tictactoe_eval.jsonl")
    return parser


def main() -> None:
    parser = make_parser()
    args = parser.parse_args()

    base_env = TicTacToeEnv(agent_starts=True)
    system_prompt = (
        (base_env.suggested_sysprompt or "")
        + "\n\nThink through the board in <think>...</think>. "
        "After </think>, output exactly one XML tag of the form <move>A1</move> and nothing else."
    )

    with maybe_start_vllm(args):
        client = VLLMChatClient(host=args.host, port=args.port, enable_weight_updates=False)
        engine = build_single_agent_engine(
            client=client,
            model=args.model,
            parser=compose_parsers(think_prefix_parser, xml_tag_parser("move", exact=True)),
            env_registry={"tictactoe": lambda agent_starts=True: TicTacToeEnv(agent_starts=agent_starts)},
            system_prompt=system_prompt,
        )
        requests = make_requests(args.episodes, args)

        def _fmt_metric(name: str, value: float) -> str:
            reducer = TICTACTOE_REDUCERS.get(name)
            if reducer is not None and reducer.as_percent:
                return f"{name}={value:.2%}"
            return f"{name}={value:.4g}"

        def _progress(done: int, total: int, stats: Dict[str, float]) -> None:
            parts = [f"[{done}/{total}]"]
            for k, v in stats.items():
                parts.append(_fmt_metric(k, v))
            print(" ".join(parts))

        records, metrics = run_eval_sync(
            engine=engine,
            requests=requests,
            reducers=TICTACTOE_REDUCERS,
            max_steps=args.max_steps,
            timeout_s=args.timeout_s,
            concurrency=args.concurrency,
            progress_every=args.concurrency,
            progress_fn=_progress,
        )

        print("\n---- Tic-Tac-Toe Evaluation ----")
        for k, v in metrics.items():
            print(_fmt_metric(k, float(v)))

        if args.out:
            write_jsonl(args.out, records)
            print(f"Wrote {len(records)} step records to {args.out}")


if __name__ == "__main__":
    main()
