How does representation salience/complexity interact with a Reward Model's (LLM backbone + Linear head) training?

Joan Velja:

I had a pesky doubt this morning on how to go about leveraging pretrained representations when using an LLM as a backbone for a Reward Model (think decoder-only layers --> extract last hidden state --> use it as input to a linear probe --> obtain a score/classification).

Joan Velja:

When training this Reward Model to score a specific feature (correctnessâ€”however defined, and task dependentâ€”, style, format, ...), does it make more sense to train the whole architecture or just the head?

I would argue (although feel free to push back, and I actually would appreciate if you did if you think I am wrong/off) that good enough/large enough RMs should be more adept to training the head only due to the linear representation hypothesis (modulo some things I will go into later).

Joan Velja:

What if this feature is not salient though? Or even worse, what if the feature is not linearly representable?

Joan Velja:

Let's formalize this. Given:

Joan Velja:

[image]

Joan Velja:

When does linear probing suffice?

So, the linear probe attachment is quite wobbly as is. First and foremost, it is usually the case that the linear head takes in as an input the last hidden state.

Nothing really forces the model to linearly represent our feature of choice in the hidden state associated to the last token, unless (i speculate) either of the following two conditions holds: i) this feature is entangled with the semantics of the token or ii) the feature has to be "kept salient" at the sequence level (think for instance a "sentiment" feature that has to be kept salient for decoding a sentiment-aligned token at the next autoregressive sampling step).

Let's still for the sake of completeness continue this argument: it is worth remembering that for large, well-trained LLMs, there's substantial empirical evidence that many features are linearly represented.

Joan Velja:

The optimal linear probe under MSE loss is:

Joan Velja:

[image]

Joan Velja:

This closed form solution is nothing but the Wiener filter: project onto the direction that maximally correlates with r*, normalized by variance structure. We do SGD on this not because this is a non-convex optimization problem, but rather because matrix inversion is computationally expensive (hh^T is a squared matrix of dimensions = hidden_state_dim) --> inversion is O(hidden_state_dim^3) (iirc)

Joan Velja:

But it's not always summer.

There's two problematic cases where the scrappy-probe-on-last-hidden-state does not work well.

Joan Velja:

Case 1: The Feature is Not Salient
"Salience" relates to the eigenspectrum of representations. Decompose:

Joan Velja:

[image]

Joan Velja:

If your feature direction aligns primarily with eigenvectors having small Î»_i, then:

Low variance along that direction means the signal is weak relative to dominant modes
The inverse Î£_h^{-1} in W* (see above) amplifies these directions, but also amplifies noise!
Joan Velja:

[image]

Joan Velja:

What happens here then? The signal might be suuuuper small, hence requiring a lot of data for W* to do a good job at figuring out how to disentangle it.

Joan Velja:

Here comes full-finetuning.

Training the backbone with reward signal backpropagates:

Joan Velja:

[image]

Joan Velja:

This reshapes the representation geometry to amplify the feature direction, increasing Î»_k (the eigenvalue associated to the feature of interest) for the relevant eigenspace. The backbone hence learns to "foreground" the previously latent/drowned feature.

Joan Velja:

Case 2: The feature is not linearly representable.

I won't go into detail here, since I think it is quite straightforward that a linear probe cannot extract non linear signal. Think of the classic XOR problem in this case.

Joan Velja:

The "Computed vs. Represented" Distinction

The thing I want you (and myself) to appreciate, is that Not all features are pre-computed in h.

Joan Velja:

Consider the question: "Is this mathematical proof correct?"

This might require:

Multi-step verification
Checking logical dependencies
Symbolic manipulation
If the model were to 0-shot this task with a probe, it would mean it is keeping track of correctness (linearly and saliently) in its hidden state! I speculate this can be induced, at least for small/easy problems, if one were to craft a prompt that requires some form of reflection, e.g.:

[PROBLEM_SPEC]

Proposed solution:

[PROPOSED_SOL]

Is correct? # <-- Take hidden state at `?` token (or last token, generally), which should be influenced by the semantics of the question

The idea is that the frozen backbone computed h for next-token prediction, not proof verification. The information might be latent in the weights but not expressed in this particular h.

Joan Velja:

Fine-tuning allows the model to learn a new computational path:

Joan Velja:

[image]

Joan Velja:

Being a LoRA fanboy in 2026

As per the title of this section, I think LoRAs give us a nice lens for elicitation of latent capabilities (I swear to god I'll write a paper about this at some point).

Why?

Joan Velja:

[image]

Joan Velja:

This says: "The feature is almost linearly representable, but we need a small correction to the representation geometry."
The rank r controls how nonlinear/complex the adjustment is. A corollary for this is that the higher the rank, the less salient the representation is. Jan Leike's elicitation project during the first Anthropic Fellows' iteration was studying something like this at some point?

Joan Velja:

Appendix
On why Full-FT trades off saliency for computational depth (this sounds horrible)
On how Full FT linearizes non-linear features

Consider the decomposition:

Joan Velja:

[image]

Joan Velja:

With frozen backbone Î¸_0, the achievable functions are:

Joan Velja:

[image]

Joan Velja:

This is an M-dimensional linear subspace of all functions ð’³ â†’ â„.

Joan Velja:

With fine-tuning, we can access:

Joan Velja:

[image]

Joan Velja:

This is vastly larger!